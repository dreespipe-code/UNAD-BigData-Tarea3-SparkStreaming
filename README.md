# UNAD-BigData-Tarea3-SparkStreaming
# UNAD â€“ Big Data  
## Tarea 3: Procesamiento de Datos con Apache Spark y Kafka  

### ğŸ“˜ DescripciÃ³n del proyecto  
Este proyecto fue desarrollado como parte de la **Tarea 3 â€“ Procesamiento de Datos con Apache Spark**, del curso **Big Data** del programa **IngenierÃ­a de Sistemas â€“ UNAD**.  
Su objetivo es implementar una soluciÃ³n de anÃ¡lisis de grandes volÃºmenes de datos utilizando **Apache Spark** y **Apache Kafka**, abordando tanto el procesamiento por lotes (*batch*) como el procesamiento en tiempo real (*streaming*).

### ğŸš€ TecnologÃ­as utilizadas  
- **Apache Spark 3.x**  
- **Apache Kafka 3.x**  
- **Python 3.10+**  
- **OpenJDK 17**  
- **HDFS / Parquet** (para almacenamiento de resultados)  

### âš™ï¸ Estructura del repositorio
UNAD-BigData-Tarea3-SparkStreaming/
â”‚
â”œâ”€ batch/
â”‚ â””â”€ etl_batch.py â†’ procesamiento batch con Spark DataFrames
â”œâ”€ streaming/
â”‚ â”œâ”€ producer.py â†’ generador de datos para Kafka
â”‚ â””â”€ streaming_job.py â†’ consumo y anÃ¡lisis en tiempo real con Spark Streaming
â”œâ”€ data/ â†’ dataset base (ejemplo: accidentes.csv)
â”œâ”€ output/ â†’ resultados generados por Spark (Parquet/JSON)
â”œâ”€ README.md â†’ este archivo descriptivo
â””â”€ LICENSE â†’ licencia MIT

### ğŸ§  Objetivos del proyecto  
1. Investigar y comparar Hadoop vs Spark, conceptos de RDDs, DataFrames y arquitectura de Kafka.  
2. Desarrollar un procesamiento **batch** con Apache Spark.  
3. Implementar un flujo de datos **streaming** usando Kafka + Spark Structured Streaming.  
4. Documentar el flujo completo de ingestiÃ³n, transformaciÃ³n y almacenamiento de datos.  
5. Generar un informe, presentaciÃ³n y video explicativo conforme a la guÃ­a de la UNAD.

### ğŸ§© Requisitos de entorno
#```bash
#sudo apt update
#sudo apt install openjdk-17-jdk python3-pip -y
#pip install pyspark==3.4.2 kafka-python
